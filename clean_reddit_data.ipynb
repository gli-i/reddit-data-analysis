{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used to load data gotten from get_reddit_data.py\n",
    "\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/30 11:45:48 WARN Utils: Your hostname, Xubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/07/30 11:45:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/30 11:45:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession, types, functions\n",
    "from pyspark.sql.functions import concat, round\n",
    "from math import sqrt\n",
    "\n",
    "spark = SparkSession.builder.appName('reddit-submissions-loader').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "assert sys.version_info >= (3, 8) # make sure we have Python 3.8+\n",
    "assert spark.version >= '3.2' # make sure we have Spark 3.2+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+--------+-----+------------+------------+-------------------+\n",
      "|     id|subreddit|               title|selftext|score|upvote_ratio|num_comments|       date_created|\n",
      "+-------+---------+--------------------+--------+-----+------------+------------+-------------------+\n",
      "|1eg07wc|  science|The proportion of...|        |   77|        0.91|           4|2024-07-30 11:05:52|\n",
      "|1efx4yw|  science|A recent study fo...|        |  235|        0.97|          20|2024-07-30 09:02:08|\n",
      "|1efvvym|  science|New transistors s...|        |  348|        0.98|          20|2024-07-30 08:13:04|\n",
      "|1eg0jk0|  science|The rocky walls o...|        |    6|         0.8|           1|2024-07-30 11:18:40|\n",
      "|1eg08al|  science|Research conducte...|        |   49|        0.95|           6|2024-07-30 11:05:52|\n",
      "+-------+---------+--------------------+--------+-----+------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_dir = 'reddit-data/comments'\n",
    "submissions_dir = 'reddit-data/submissions'\n",
    "\n",
    "# to try it out with more controversial/popular subreddits\n",
    "# comments_dir = 'reddit-subset/comments'\n",
    "# submissions_dir = 'reddit-subset/submissions'\n",
    "\n",
    "subs_schema = types.StructType([\n",
    "    types.StructField('id', types.StringType()),\n",
    "    types.StructField('subreddit', types.StringType()),\n",
    "    types.StructField('title', types.StringType()),\n",
    "    types.StructField('selftext', types.StringType()),\n",
    "    types.StructField('score', types.LongType()),\n",
    "    types.StructField('upvote_ratio', types.FloatType()),\n",
    "    types.StructField('num_comments', types.LongType()),\n",
    "    types.StructField('date_created', types.TimestampType()),\n",
    "])\n",
    "comms_schema = types.StructType([\n",
    "    types.StructField('link_id', types.StringType()),\n",
    "    types.StructField('body', types.StringType()),\n",
    "    types.StructField('score', types.LongType()),\n",
    "    types.StructField('date_created', types.TimestampType())\n",
    "])\n",
    "comments_df = spark.read.json(comments_dir, schema=comms_schema)\n",
    "submissions_df = spark.read.json(submissions_dir, schema=subs_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some comments dont have a link_id, so we can remove them \n",
    "#since we wont be able to link back to the post on which they commneted \n",
    "    \n",
    "#some submissions also have a null score, upvote_ratio, title and body which is needed\n",
    "comments_df = comments_df.dropna()\n",
    "submissions_df = submissions_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/gli/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(['stopwords', 'vader_lexicon', 'punkt', 'wordnet'])\n",
    "\n",
    "# initialize NLTK sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stopwords_eng = stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords_eng]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "sentiment_udf = functions.udf(sentiment_analysis,\n",
    "                        returnType=types.FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df = submissions_df.withColumn('sentiment_score', sentiment_udf(concat(submissions_df['title'], submissions_df['selftext'])))\n",
    "comments_df = comments_df.withColumn('sentiment_score', sentiment_udf(comments_df['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no popularity score for comments, so it can be written now\n",
    "comments_df.write.format(\"csv\").save(\"comments_cleaned\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of up and downvotes\n",
    "# formula for downvotes: d = (s*(1-r)) / (2r-1)\n",
    "# we get this from the 2 formulas: score = s = u-d and upvote_ratio = r = u/(u+d)\n",
    "\n",
    "submissions_df = submissions_df.withColumn('downs', round(submissions_df['score']*(1-submissions_df['upvote_ratio']) / (2*submissions_df['upvote_ratio']-1)))\n",
    "submissions_df = submissions_df.withColumn('ups', submissions_df['score'] + submissions_df['downs'])\n",
    "submissions_df = submissions_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(ups, downs):\n",
    "    # from https://stackoverflow.com/a/10029645 which adapted this from https://medium.com/hacking-and-gonzo/how-reddit-ranking-algorithms-work-ef111e33d0d9\n",
    "    # this uses the wilson score interval, which takes into account both the ratio of ups/downs and the total number of them\n",
    "        # so a post with only 1 upvote would rank lower than say a post with 10 upvotes and 2 downvotes, even though it has a higher ratio\n",
    "        # the float output is the probability of the next person who sees posts upvoting it (as opposed to downvoting) (?)\n",
    "    # ^ my understanding of this, will research more for final paper if we do use this\n",
    "    n = ups + downs\n",
    "\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    z = 1.0 #1.44 = 85%, 1.96 = 95%\n",
    "    phat = float(ups) / n\n",
    "    return ((phat + z*z/(2*n) - z * sqrt((phat*(1-phat)+z*z/(4*n))/n))/(1+z*z/n))\n",
    "    \n",
    "confidence_udf = functions.udf(confidence,\n",
    "                        returnType=types.FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df = submissions_df.withColumn('pop_score', confidence_udf(submissions_df['ups'], submissions_df['downs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "submissions_df = submissions_df.select('id', 'subreddit', 'title', 'num_comments', 'sentiment_score', 'pop_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "submissions_df.write.format(\"csv\").save(\"submissions_cleaned\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
