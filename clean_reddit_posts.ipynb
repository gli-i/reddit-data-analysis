{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used to load data gotten from get_reddit_data.py\n",
    "\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession, types, functions\n",
    "from pyspark.sql.functions import concat, round, date_format, monotonically_increasing_id \n",
    "from pyspark.sql.types import StringType\n",
    "from math import sqrt\n",
    "\n",
    "spark = SparkSession.builder.appName('reddit-submissions-loader').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "assert sys.version_info >= (3, 8) # make sure we have Python 3.8+\n",
    "assert spark.version >= '3.2' # make sure we have Spark 3.2+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_dir = 'reddit-data/submissions'\n",
    "\n",
    "subs_schema = types.StructType([\n",
    "    types.StructField('id', types.StringType()),\n",
    "    types.StructField('subreddit', types.StringType()),\n",
    "    types.StructField('title', types.StringType()),\n",
    "    types.StructField('selftext', types.StringType()),\n",
    "    types.StructField('score', types.LongType()),\n",
    "    types.StructField('upvote_ratio', types.FloatType()),\n",
    "    types.StructField('num_comments', types.LongType()),\n",
    "    types.StructField('date_created', types.TimestampType()),\n",
    "])\n",
    "\n",
    "submissions_df = spark.read.json(submissions_dir, schema=subs_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some comments dont have a link_id, so we can remove them \n",
    "#since we wont be able to link back to the post on which they commneted \n",
    "    \n",
    "#some submissions also have a null score, upvote_ratio, title and body which is needed\n",
    "submissions_df = submissions_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn timestamps into month and time of day\n",
    "\n",
    "submissions_df = submissions_df.withColumn('time_created', date_format('date_created', 'HH:mm:ss'))\n",
    "submissions_df = submissions_df.drop('date_created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/gli/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(['stopwords', 'vader_lexicon', 'punkt', 'wordnet'])\n",
    "\n",
    "# initialize NLTK sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stopwords_eng = stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords_eng]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "sentiment_udf = functions.udf(sentiment_analysis,\n",
    "                        returnType=types.FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df = submissions_df.withColumn('sentiment_score', sentiment_udf(concat(submissions_df['title'], submissions_df['selftext'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of up and downvotes\n",
    "# formula for downvotes: d = (s*(1-r)) / (2r-1)\n",
    "# we get this from the 2 formulas: score = s = u-d and upvote_ratio = r = u/(u+d)\n",
    "\n",
    "submissions_df = submissions_df.withColumn('downs', round(submissions_df['score']*(1-submissions_df['upvote_ratio']) / (2*submissions_df['upvote_ratio']-1)))\n",
    "submissions_df = submissions_df.withColumn('ups', submissions_df['score'] + submissions_df['downs'])\n",
    "submissions_df = submissions_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(ups, downs):\n",
    "    # from https://stackoverflow.com/a/10029645 which adapted this from https://medium.com/hacking-and-gonzo/how-reddit-ranking-algorithms-work-ef111e33d0d9\n",
    "    # this uses the wilson score interval, which takes into account both the ratio of ups/downs and the total number of them\n",
    "        # so a post with only 1 upvote would rank lower than say a post with 10 upvotes and 2 downvotes, even though it has a higher ratio\n",
    "        # the float output is the probability of the next person who sees posts upvoting it (as opposed to downvoting) (?)\n",
    "    # ^ my understanding of this, will research more for final paper if we do use this\n",
    "    n = ups + downs\n",
    "\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    z = 1.0 #1.44 = 85%, 1.96 = 95%\n",
    "    phat = float(ups) / n\n",
    "    return ((phat + z*z/(2*n) - z * sqrt((phat*(1-phat)+z*z/(4*n))/n))/(1+z*z/n))\n",
    "    \n",
    "confidence_udf = functions.udf(confidence,\n",
    "                        returnType=types.FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df = submissions_df.withColumn('pop_score', confidence_udf(submissions_df['ups'], submissions_df['downs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for if controversial or not\n",
    "submissions_df = submissions_df.withColumn('controversial', submissions_df['upvote_ratio'] < 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "submissions_df = submissions_df.select('id', 'subreddit', 'title', 'time_created', 'num_comments', 'sentiment_score', 'pop_score', 'controversial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "submissions_df.write.format(\"csv\").save(\"submissions_cleaned\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
